{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are Attention Mechanisms Important?\n",
    "## Attention mechanisms have become indispensable in various deep-learning applications due to their ability to address some critical challenges:\n",
    "\n",
    "* Long Sequences: Traditional neural networks struggle with processing long sequences, such as translating a paragraph from one language to another. Attention mechanisms allow models to focus on the relevant parts of the input, making them more effective at handling lengthy data.\n",
    "* Contextual Understanding: In tasks like language translation, understanding the context of a word is crucial for accurate translation. Attention mechanisms enable models to consider the context by assigning different attention weights to each word in the input sequence.\n",
    "* Improved Performance: Models equipped with attention mechanisms often outperform their non-attention counterparts. They achieve state-of-the-art results in tasks like machine translation, image classification, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The name \"transformer\" for models using attention layers comes from the influential paper \"Attention Is All You Need\" by Vaswani et al. (2017). The term \"transformer\" reflects the model's architecture and its ability to \"transform\" sequences of data through a series of attention-based layers.\n",
    "\n",
    "## Here are key reasons why the name \"transformer\" was chosen:\n",
    "\n",
    "* Attention Mechanism: The core innovation of the transformer model is the self-attention mechanism. This allows the model to weigh the importance of different words in a sentence, regardless of their position, enabling it to capture long-range dependencies and relationships more effectively than previous models.\n",
    "\n",
    "* Sequence Transformation: The model transforms input sequences into output sequences. In the context of natural language processing (NLP), this means transforming an input sentence into an output sentence, such as translating from one language to another.\n",
    "\n",
    "* Layered Architecture: Transformers are composed of multiple layers of self-attention and feed-forward neural networks. Each layer processes the input and transforms it into a more abstract representation, progressively refining the output through successive transformations.\n",
    "\n",
    "* Scalability: The architecture is highly scalable, allowing for the use of large amounts of data and computational resources to improve performance. This transformation of data at scale is a key feature of the model.\n",
    "\n",
    "* Shift from RNNs/CNNs: Prior to transformers, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) were commonly used for sequence modeling. The transformer model represents a significant shift in approach, using attention mechanisms without recurrence or convolution, thus transforming the field of deep learning.\n",
    "\n",
    "In summary, the name \"transformer\" encapsulates the model's ability to apply attention mechanisms to transform input sequences into more useful representations, enabling powerful and flexible modeling of sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example in Machine Translation\n",
    "* https://medium.com/@zhonghong9998/attention-mechanisms-in-deep-learning-enhancing-model-performance-32a91006092a\n",
    "* https://www.scaler.com/topics/deep-learning/attention-mechanism-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Size\n",
    "### Context size is a huge bottleneck for language models because its o(n^2), K(n)*Q(n) so these are some approches for tackling this limit\n",
    "* Sparse Attention Mechanism\n",
    "* Blockwise Attention\n",
    "* Linformer\n",
    "* Reformer\n",
    "* Ring attention\n",
    "* Longformer\n",
    "* Adaptive Attention Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention(Q, K, V ) = softmax(Q*KT/âˆšdk)*V\n",
    "## Multi-Head computes multiple of this table, so each head goes for finding a pattern in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Attention from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder representations of four different words\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the weight matrices\n",
    "random.seed(42) # to allow us to reproduce the same attention values\n",
    "W_Q = random.randint(3, size=(3, 3))\n",
    "W_K = random.randint(3, size=(3, 3))\n",
    "W_V = random.randint(3, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = array([word_1, word_2, word_3, word_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = words @ W_Q\n",
    "K = words @ W_K\n",
    "V = words @ W_V\n",
    " \n",
    "# scoring the query vectors against all key vectors\n",
    "scores = Q @ K.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2, 10,  2],\n",
       "       [ 4,  0,  4,  0],\n",
       "       [12,  2, 14,  2],\n",
       "       [10,  4, 14,  3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98522025, 1.74174051, 0.75652026],\n",
       "       [0.90965265, 1.40965265, 0.5       ],\n",
       "       [0.99851226, 1.75849334, 0.75998108],\n",
       "       [0.99560386, 1.90407309, 0.90846923]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.98522025, 1.74174051, 0.75652026],\n",
       "       [0.90965265, 2.40965265, 0.5       ],\n",
       "       [1.99851226, 2.75849334, 0.75998108],\n",
       "       [0.99560386, 1.90407309, 1.90846923]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newEmbbeding = attention + words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Vars\n",
    "TOP_WORDS = 5000\n",
    "EMBEDDING_LEN = 32\n",
    "ADD_ATTENTION = True\n",
    "MAX_INPUT_LEN = 200\n",
    "\n",
    "# Preprocess the data\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.imdb.load_data(num_words=TOP_WORDS)\n",
    "train_x = tf.keras.preprocessing.sequence.pad_sequences(train_x, maxlen=MAX_INPUT_LEN)\n",
    "test_x = tf.keras.preprocessing.sequence.pad_sequences(test_x, maxlen=MAX_INPUT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 - 20s - 50ms/step - accuracy: 0.6713 - loss: 0.6404 - val_accuracy: 0.7602 - val_loss: 0.5315\n",
      "Epoch 2/10\n",
      "391/391 - 5s - 14ms/step - accuracy: 0.8074 - loss: 0.4582 - val_accuracy: 0.8425 - val_loss: 0.3943\n",
      "Epoch 3/10\n",
      "391/391 - 5s - 14ms/step - accuracy: 0.8465 - loss: 0.3761 - val_accuracy: 0.8551 - val_loss: 0.3474\n",
      "Epoch 4/10\n",
      "391/391 - 6s - 14ms/step - accuracy: 0.8610 - loss: 0.3403 - val_accuracy: 0.8692 - val_loss: 0.3228\n",
      "Epoch 5/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8717 - loss: 0.3140 - val_accuracy: 0.8734 - val_loss: 0.3099\n",
      "Epoch 6/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8818 - loss: 0.2967 - val_accuracy: 0.8758 - val_loss: 0.3006\n",
      "Epoch 7/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8878 - loss: 0.2838 - val_accuracy: 0.8780 - val_loss: 0.2957\n",
      "Epoch 8/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8927 - loss: 0.2729 - val_accuracy: 0.8735 - val_loss: 0.3001\n",
      "Epoch 9/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8950 - loss: 0.2650 - val_accuracy: 0.8747 - val_loss: 0.2978\n",
      "Epoch 10/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.9002 - loss: 0.2576 - val_accuracy: 0.8803 - val_loss: 0.2900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d730fa66c0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "inputs = keras.layers.Input(shape=(MAX_INPUT_LEN,))\n",
    "embedding = keras.layers.Embedding(TOP_WORDS, EMBEDDING_LEN, input_length=MAX_INPUT_LEN)(inputs)\n",
    "x = keras.layers.Dropout(0.5)(embedding)\n",
    "\n",
    "if ADD_ATTENTION:\n",
    "    # lstm_out = keras.layers.LSTM(100, return_sequences=True)(x)\n",
    "    query_value_attention_seq = keras.layers.Attention()([x, x]) # using same key as query casue we are using self-attention\n",
    "    x = keras.layers.GlobalAveragePooling1D()(query_value_attention_seq)\n",
    "else:\n",
    "    x = keras.layers.LSTM(100)(x)\n",
    "    x = keras.layers.Dense(350, activation='relu')(x)\n",
    "\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_x, train_y,\n",
    "    verbose=2,\n",
    "    validation_data=(test_x, test_y),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_TEXT_MAKE.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PARATCO\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 - 49s - 126ms/step - accuracy: 0.7513 - loss: 0.4848 - val_accuracy: 0.8570 - val_loss: 0.3439\n",
      "Epoch 2/10\n",
      "391/391 - 34s - 87ms/step - accuracy: 0.8758 - loss: 0.3086 - val_accuracy: 0.8692 - val_loss: 0.3091\n",
      "Epoch 3/10\n",
      "391/391 - 61s - 156ms/step - accuracy: 0.8916 - loss: 0.2709 - val_accuracy: 0.8738 - val_loss: 0.2989\n",
      "Epoch 4/10\n",
      "391/391 - 62s - 159ms/step - accuracy: 0.9042 - loss: 0.2425 - val_accuracy: 0.8678 - val_loss: 0.3278\n",
      "Epoch 5/10\n",
      "391/391 - 38s - 96ms/step - accuracy: 0.9110 - loss: 0.2285 - val_accuracy: 0.8709 - val_loss: 0.3089\n",
      "Epoch 6/10\n",
      "391/391 - 28s - 71ms/step - accuracy: 0.9204 - loss: 0.2042 - val_accuracy: 0.8634 - val_loss: 0.3233\n",
      "Epoch 7/10\n",
      "391/391 - 27s - 70ms/step - accuracy: 0.9266 - loss: 0.1889 - val_accuracy: 0.8684 - val_loss: 0.3595\n",
      "Epoch 8/10\n",
      "391/391 - 28s - 71ms/step - accuracy: 0.9315 - loss: 0.1742 - val_accuracy: 0.8658 - val_loss: 0.3826\n",
      "Epoch 9/10\n",
      "391/391 - 27s - 69ms/step - accuracy: 0.9318 - loss: 0.1729 - val_accuracy: 0.8642 - val_loss: 0.3853\n",
      "Epoch 10/10\n",
      "391/391 - 27s - 69ms/step - accuracy: 0.9394 - loss: 0.1560 - val_accuracy: 0.8653 - val_loss: 0.3560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d72e08e6f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(TOP_WORDS, EMBEDDING_LEN, input_length=MAX_INPUT_LEN),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    *([tf.keras.layers.LSTM(100, return_sequences=True), keras.layers.Attention()] if ADD_ATTENTION\n",
    "        else [tf.keras.layers.LSTM(100), tf.keras.layers.Dense(350, activation='relu')]),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_x, train_y,\n",
    "    verbose=2,\n",
    "    validation_data=(test_x, test_y),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention is much faster and a bit more acurate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
