{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example in Machine Translation\n",
    "* https://medium.com/@zhonghong9998/attention-mechanisms-in-deep-learning-enhancing-model-performance-32a91006092a\n",
    "* https://www.scaler.com/topics/deep-learning/attention-mechanism-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention(Q, K, V ) = softmax(Q*KT/âˆšdk)*V\n",
    "## Multi-Head computes multiple of this table, so each head goes for finding a pattern in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Attention from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder representations of four different words\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the weight matrices\n",
    "random.seed(42) # to allow us to reproduce the same attention values\n",
    "W_Q = random.randint(3, size=(3, 3))\n",
    "W_K = random.randint(3, size=(3, 3))\n",
    "W_V = random.randint(3, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = array([word_1, word_2, word_3, word_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = words @ W_Q\n",
    "K = words @ W_K\n",
    "V = words @ W_V\n",
    " \n",
    "# scoring the query vectors against all key vectors\n",
    "scores = Q @ K.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2, 10,  2],\n",
       "       [ 4,  0,  4,  0],\n",
       "       [12,  2, 14,  2],\n",
       "       [10,  4, 14,  3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98522025, 1.74174051, 0.75652026],\n",
       "       [0.90965265, 1.40965265, 0.5       ],\n",
       "       [0.99851226, 1.75849334, 0.75998108],\n",
       "       [0.99560386, 1.90407309, 0.90846923]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.98522025, 1.74174051, 0.75652026],\n",
       "       [0.90965265, 2.40965265, 0.5       ],\n",
       "       [1.99851226, 2.75849334, 0.75998108],\n",
       "       [0.99560386, 1.90407309, 1.90846923]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newEmbbeding = attention + words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Vars\n",
    "TOP_WORDS = 5000\n",
    "EMBEDDING_LEN = 32\n",
    "ADD_ATTENTION = True\n",
    "MAX_INPUT_LEN = 200\n",
    "\n",
    "# Preprocess the data\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.imdb.load_data(num_words=TOP_WORDS)\n",
    "train_x = tf.keras.preprocessing.sequence.pad_sequences(train_x, maxlen=MAX_INPUT_LEN)\n",
    "test_x = tf.keras.preprocessing.sequence.pad_sequences(test_x, maxlen=MAX_INPUT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 - 20s - 50ms/step - accuracy: 0.6713 - loss: 0.6404 - val_accuracy: 0.7602 - val_loss: 0.5315\n",
      "Epoch 2/10\n",
      "391/391 - 5s - 14ms/step - accuracy: 0.8074 - loss: 0.4582 - val_accuracy: 0.8425 - val_loss: 0.3943\n",
      "Epoch 3/10\n",
      "391/391 - 5s - 14ms/step - accuracy: 0.8465 - loss: 0.3761 - val_accuracy: 0.8551 - val_loss: 0.3474\n",
      "Epoch 4/10\n",
      "391/391 - 6s - 14ms/step - accuracy: 0.8610 - loss: 0.3403 - val_accuracy: 0.8692 - val_loss: 0.3228\n",
      "Epoch 5/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8717 - loss: 0.3140 - val_accuracy: 0.8734 - val_loss: 0.3099\n",
      "Epoch 6/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8818 - loss: 0.2967 - val_accuracy: 0.8758 - val_loss: 0.3006\n",
      "Epoch 7/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8878 - loss: 0.2838 - val_accuracy: 0.8780 - val_loss: 0.2957\n",
      "Epoch 8/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8927 - loss: 0.2729 - val_accuracy: 0.8735 - val_loss: 0.3001\n",
      "Epoch 9/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.8950 - loss: 0.2650 - val_accuracy: 0.8747 - val_loss: 0.2978\n",
      "Epoch 10/10\n",
      "391/391 - 3s - 7ms/step - accuracy: 0.9002 - loss: 0.2576 - val_accuracy: 0.8803 - val_loss: 0.2900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d730fa66c0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "inputs = keras.layers.Input(shape=(MAX_INPUT_LEN,))\n",
    "embedding = keras.layers.Embedding(TOP_WORDS, EMBEDDING_LEN, input_length=MAX_INPUT_LEN)(inputs)\n",
    "x = keras.layers.Dropout(0.5)(embedding)\n",
    "\n",
    "if ADD_ATTENTION:\n",
    "    # lstm_out = keras.layers.LSTM(100, return_sequences=True)(x)\n",
    "    query_value_attention_seq = keras.layers.Attention()([x, x]) # using same key as query casue we are using self-attention\n",
    "    x = keras.layers.GlobalAveragePooling1D()(query_value_attention_seq)\n",
    "else:\n",
    "    x = keras.layers.LSTM(100)(x)\n",
    "    x = keras.layers.Dense(350, activation='relu')(x)\n",
    "\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_x, train_y,\n",
    "    verbose=2,\n",
    "    validation_data=(test_x, test_y),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_TEXT_MAKE.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PARATCO\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 - 49s - 126ms/step - accuracy: 0.7513 - loss: 0.4848 - val_accuracy: 0.8570 - val_loss: 0.3439\n",
      "Epoch 2/10\n",
      "391/391 - 34s - 87ms/step - accuracy: 0.8758 - loss: 0.3086 - val_accuracy: 0.8692 - val_loss: 0.3091\n",
      "Epoch 3/10\n",
      "391/391 - 61s - 156ms/step - accuracy: 0.8916 - loss: 0.2709 - val_accuracy: 0.8738 - val_loss: 0.2989\n",
      "Epoch 4/10\n",
      "391/391 - 62s - 159ms/step - accuracy: 0.9042 - loss: 0.2425 - val_accuracy: 0.8678 - val_loss: 0.3278\n",
      "Epoch 5/10\n",
      "391/391 - 38s - 96ms/step - accuracy: 0.9110 - loss: 0.2285 - val_accuracy: 0.8709 - val_loss: 0.3089\n",
      "Epoch 6/10\n",
      "391/391 - 28s - 71ms/step - accuracy: 0.9204 - loss: 0.2042 - val_accuracy: 0.8634 - val_loss: 0.3233\n",
      "Epoch 7/10\n",
      "391/391 - 27s - 70ms/step - accuracy: 0.9266 - loss: 0.1889 - val_accuracy: 0.8684 - val_loss: 0.3595\n",
      "Epoch 8/10\n",
      "391/391 - 28s - 71ms/step - accuracy: 0.9315 - loss: 0.1742 - val_accuracy: 0.8658 - val_loss: 0.3826\n",
      "Epoch 9/10\n",
      "391/391 - 27s - 69ms/step - accuracy: 0.9318 - loss: 0.1729 - val_accuracy: 0.8642 - val_loss: 0.3853\n",
      "Epoch 10/10\n",
      "391/391 - 27s - 69ms/step - accuracy: 0.9394 - loss: 0.1560 - val_accuracy: 0.8653 - val_loss: 0.3560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d72e08e6f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(TOP_WORDS, EMBEDDING_LEN, input_length=MAX_INPUT_LEN),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    *([tf.keras.layers.LSTM(100, return_sequences=True), keras.layers.Attention()] if ADD_ATTENTION\n",
    "        else [tf.keras.layers.LSTM(100), tf.keras.layers.Dense(350, activation='relu')]),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_x, train_y,\n",
    "    verbose=2,\n",
    "    validation_data=(test_x, test_y),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention is much faster and a bit more acurate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
