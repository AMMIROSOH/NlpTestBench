{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>  Mixture of Experts </strong>\n",
    "\n",
    "The paper \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" introduces Mamba, a novel sequence modeling architecture designed to address the computational inefficiencies of Transformers, especially when handling long sequences.\n",
    "\n",
    "* Traditional Transformers, which underpin many deep learning applications, face challenges with long sequences due to their quadratic computational complexity and limited context window. While various subquadratic-time architectures have been proposed, they often underperform in key areas like language processing. The authors identify that a significant weakness of these models is their inability to perform content-based reasoning, it seems that mamaba is 5 times faster on inference throughput vs Best transformers.\n",
    "\n",
    "* MAMBA Computation grows linearly and Transformers grow Expontially.\n",
    "\n",
    "* Unlike LSTMs that each block should wait for output of previous Block, MAMBAs gets a global hidden state value and blocks dont need to wait for eachother.\n",
    "\n",
    "> note: <strong> mamba is a mixup of LSTMs and Attentions that is inherited from a old Sequence Modeling that were not being used these days. </strong>\n",
    "\n",
    "* A Survey on Mixture of Experts\n",
    "* https://arxiv.org/pdf/2407.06204\n",
    "* VMamba: Visual State Space Model\n",
    "* https://arxiv.org/abs/2401.10166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) in Neural Networks\n",
    "\n",
    "Mixture of Experts (MoE) is an advanced neural network architecture that combines multiple specialized sub-models, known as \"experts,\" to address complex tasks by partitioning them into more manageable sub-tasks. A gating network determines the contribution of each expert for a given input, enabling the model to leverage specialized knowledge effectively.\n",
    "\n",
    "## Architecture and Functionality\n",
    "\n",
    "An MoE model comprises two primary components:\n",
    "\n",
    "1. **Experts**: Individual neural networks, denoted as \\( f_i(x) \\), each trained to specialize in different regions of the input space.\n",
    "\n",
    "2. **Gating Network**: A neural network, \\( g(x) \\), that analyzes each input \\( x \\) and assigns a weight \\( g_i(x) \\) to each expert, indicating its relevance to the given input.\n",
    "\n",
    "The model processes an input \\( x \\) as follows:\n",
    "\n",
    "- The gating network evaluates the input and produces a set of weights corresponding to each expert:\n",
    "\n",
    "  \\[ g_i(x) = \\frac{\\exp(h_i(x))}{\\sum_{j=1}^N \\exp(h_j(x))} \\]\n",
    "\n",
    "  where \\( h_i(x) \\) is the output of the gating network before the softmax function, and \\( N \\) is the total number of experts.\n",
    "\n",
    "- Each expert processes the input independently, generating an output \\( f_i(x) \\).\n",
    "\n",
    "- The final output \\( y \\) is a weighted sum of the experts' outputs:\n",
    "\n",
    "  \\[ y = \\sum_{i=1}^N g_i(x) f_i(x) \\]\n",
    "\n",
    "This architecture allows the MoE model to adaptively select and combine the most pertinent experts for each input, enhancing both efficiency and performance.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "In probabilistic terms, MoE can be viewed as modeling the conditional probability distribution \\( P(y \\mid x) \\) as a mixture model:\n",
    "\n",
    "\\[ P(y \\mid x) = \\sum_{i=1}^N P(y \\mid x, z=i) P(z=i \\mid x) \\]\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( P(z=i \\mid x) \\) is the gating network's output, representing the probability of selecting the \\( i \\)-th expert given input \\( x \\).\n",
    "\n",
    "- \\( P(y \\mid x, z=i) \\) is the output of the \\( i \\)-th expert, representing the conditional probability of \\( y \\) given \\( x \\) and that the \\( i \\)-th expert is selected.\n",
    "\n",
    "## Advantages and Disadvantages\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- **Scalability**: MoE architectures can scale to accommodate a large number of experts, each specializing in different sub-tasks, facilitating the handling of complex problems.\n",
    "\n",
    "- **Efficiency**: By activating only a subset of experts for each input, MoE models can increase model capacity without a proportional increase in computational cost. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "- **Flexibility**: The modular nature of MoE allows for the addition or removal of experts without necessitating a complete retraining of the model.\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- **Complexity**: Implementing MoE models introduces additional complexity in terms of architecture design and training procedures.\n",
    "\n",
    "- **Load Balancing**: Ensuring that all experts are utilized effectively can be challenging, as some experts may become overburdened while others are underutilized.\n",
    "\n",
    "- **Training Stability**: Training MoE models can be less stable compared to traditional neural networks, requiring careful tuning of hyperparameters and optimization strategies.\n",
    "\n",
    "## Implementation Strategies\n",
    "\n",
    "Implementing an MoE model involves several key steps:\n",
    "\n",
    "1. **Designing Experts**: Develop multiple neural networks, each tailored to specialize in a specific aspect of the input space.\n",
    "\n",
    "2. **Constructing the Gating Network**: Create a gating network that can assess inputs and assign appropriate weights to each expert.\n",
    "\n",
    "3. **Training**: Train the experts and the gating network simultaneously or iteratively, ensuring that the gating network learns to assign inputs to the most suitable experts.\n",
    "\n",
    "4. **Integration**: Combine the outputs of the experts based on the weights provided by the gating network to produce the final output.\n",
    "\n",
    "## Special Techniques in MoE\n",
    "\n",
    "- **Sparsely-Gated MoE**: This technique involves activating only the top-\\( k \\) experts for each input, reducing computational requirements and improving efficiency. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "- **Hash MoE**: Routing is performed deterministically by a hash function, fixed before learning begins. For example, if the model is a 4-layered Transformer, and the input is a token for the word \"eat,\" and the hash of \"eat\" is (1, 4, 2, 3), then the token would be routed to the 1st expert in layer 1, 4th expert in layer 2, etc. Despite its simplicity, it achieves competitive performance as sparsely gated MoE with \\( k = 1 \\). :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "- **Soft MoE**: In this approach, each expert processes a weighted combination of all inputs, allowing for a more flexible allocation of computational resources. However, this does not work with autoregressive modeling, since the weights over one token depend on all other tokens. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "- **Load Balancing Strategies**: Implementing auxiliary loss functions or regularization techniques can help distribute the workload evenly among experts, preventing some from becoming overutilized while others remain idle.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Mixture of Experts is a powerful architecture in neural networks that enhances performance and scalability by combining specialized models. While it introduces additional complexity and potential challenges in training and load balancing, the benefits often outweigh the drawbacks, particularly for complex tasks requiring specialized knowledge.\n",
    "\n",
    "For a more in-depth understanding, consider exploring the following resources:\n",
    "\n",
    "- [A Survey on Mixture of Experts](https://arxiv.org/abs/2407.06204)\n",
    "\n",
    "- [Mixture of\n",
    "::contentReference[oaicite:4]{index=4}\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
