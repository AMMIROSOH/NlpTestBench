{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ed3120",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Enhanced Fake News Detection with Web-Based Evidence Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab360b",
   "metadata": {},
   "source": [
    "This notebook builds a fake-news detection pipeline combining a Large Language Model (LLM) with\n",
    "web-based evidence retrieval. We will: (1) retrieve relevant evidence via the Google Custom Search API,\n",
    "(2) summarize and rank the evidence, (3) prompt an OpenAI GPT model (3.5/4) using a chain-ofthought prompt that includes the evidence, and (4) output a final truth/fake label along with the\n",
    "model’s reasoning. We break the solution into major modules to keep the code organized. Relevant\n",
    "references are cited throughout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50e0f1",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "We install and import necessary libraries. We use the OpenAI API for chain-of-thought reasoning, the\n",
    "Google API client for search, Hugging Face Transformers for summarization, and SentenceTransformers for semantic similarity ranking. We also configure API keys (placeholders shown below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e70c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\paratco\\anaconda3\\lib\\site-packages (1.57.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\paratco\\anaconda3\\lib\\site-packages (2.154.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\paratco\\anaconda3\\lib\\site-packages (4.43.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sentencetransformers (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for sentencetransformers\n"
     ]
    }
   ],
   "source": [
    "!pip install openai google-api-python-client transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93329598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from googleapiclient.discovery import build\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "openai.api_key = \"\" #\"OPENAI_API_KEY\"\n",
    "google_api_key = \"\" # \"GOOGLE_API_KEY\"\n",
    "google_cse_id = \"\" # \"GOOGLE_CSE_ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6193dd0",
   "metadata": {},
   "source": [
    "## Tools Used\n",
    "* OpenAI API: We will use openai.ChatCompletion.create() with models like \"gpt-4\".\n",
    "* Google Custom Search API: We use googleapiclient.discovery.build as in [StackOverflow ] to perform web searches.\n",
    "* Hugging Face Transformers: A pipeline(\"summarization\") can quickly summarize text snippets.\n",
    "* Sentence-Transformers: We use a model like all-MiniLM-L6-v2 to embed text and compute cosine similarity for ranking evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d05a6",
   "metadata": {},
   "source": [
    "# 1: Make a Google Search Query from the Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b11a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_query(claim: str, \n",
    "                          model: str = \"gpt-4o-mini\", \n",
    "                          temperature: float = 0.0, \n",
    "                          max_tokens: int = 50) -> str:\n",
    "    system_prompt = (\n",
    "        \"You are an expert web‑search query generator. \"\n",
    "        \"Your goal is to convert a factual claim into a succinct, \"\n",
    "        \"keyword‑rich search query (5–10 words max), \"\n",
    "        \"using quoted phrases and Boolean operators if useful. \"\n",
    "        \"Only output the query text—no explanations or extra text.\"\n",
    "    )\n",
    "    user_prompt = f\"Claim: “{claim}”\\nGenerate the best search query for fact‑checking this.\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip().strip(\"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2b865",
   "metadata": {},
   "source": [
    "# 2: Evidence Retrieval from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2a42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query, api_key, cse_id, num=5):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=query, cx=cse_id, num=num).execute()\n",
    "    return res.get(\"items\", [])\n",
    "def retrieve_evidence(claim, top_k=5):\n",
    "    results = google_search(claim, google_api_key, google_cse_id, num=top_k)\n",
    "    evidence = []\n",
    "    for item in results:\n",
    "        title = item.get('title')\n",
    "        snippet = item.get('snippet')\n",
    "        link = item.get('link')\n",
    "        if snippet:\n",
    "            evidence.append((title, snippet, link))\n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ef9e2",
   "metadata": {},
   "source": [
    "* We search the entire web but can bias toward reliable news/fact-check sites by adding filters (e.g. site:politiFact.com or site:snopes.com).\n",
    "* The retrieve_evidence function returns raw text snippets from search results to be used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660d3efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Trump calls for deal on Israel war in Gaza amid signs of progress ...',\n",
       "  '22 hours ago ... President Trump pleaded for progress in ceasefire talks in the war in Gaza, as Israel and Hamas appeared to be inching closer to an\\xa0...',\n",
       "  'https://www.npr.org/2025/06/29/nx-s1-5450133/trump-gaza-netanyahu'),\n",
       " ('HEALTH, EDUCATION, LABOR AND PENSIONS COMMITTEE ...',\n",
       "  \"May 13, 2025 ... Trump's war on science is not making America healthy again. It is making Americans and people throughout the world sicker. 4 Interview with\\xa0...\",\n",
       "  'https://www.sanders.senate.gov/wp-content/uploads/HELP-Committee-Minority-Report-Trumps-War-on-Science.pdf'),\n",
       " ('Trump, Congress, and the War Powers Resolution | The New Yorker',\n",
       "  \"2 days ago ... These worries have intensified in debates about the legality of President Trump's decision to bomb Iranian nuclear facilities more than a week\\xa0...\",\n",
       "  'https://www.newyorker.com/magazine/2025/07/07/trump-congress-and-the-war-powers-resolution'),\n",
       " ('Senate votes down measure restricting Trump from further military ...',\n",
       "  \"2 days ago ... Tim Kaine of Virginia introduced the war powers resolution days before the U.S. bombed three locations central to Iran's nuclear program,\\xa0...\",\n",
       "  'https://www.cbsnews.com/news/senate-trump-war-powers-iran-strikes/'),\n",
       " ('Trump Signs Proclamation Commemorating End of Korean War ...',\n",
       "  'Jul 25, 2020 ... The guns fell silent along the Korean demilitarized zone 67 years ago, after more than three years of brutal fighting to defeat the\\xa0...',\n",
       "  'https://www.defense.gov/News/News-Stories/Article/Article/2288357/trump-signs-proclamation-commemorating-end-of-korean-war/')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_evidence(\"trump war\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d49a74",
   "metadata": {},
   "source": [
    "# 3: Summarization and Ranking of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "048e028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def chunk_text(text, max_tokens=3500, model=\"gpt-4o-mini\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = text.split()\n",
    "    chunks, current = [], []\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        count += len(enc.encode(word))\n",
    "        if count > max_tokens:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [word]\n",
    "            count = len(enc.encode(word))\n",
    "        else:\n",
    "            current.append(word)\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "def openai_summarize(text, max_tokens=150):\n",
    "    chunks = chunk_text(text)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize this in 2‑3 sentences:\\n\\n{chunk}\"}\n",
    "            ]\n",
    "        )\n",
    "        summaries.append(resp.choices[0].message.content.strip())\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "def summarize_snippets(evidence):\n",
    "    summaries = []\n",
    "    for title, snippet, link in evidence:\n",
    "        summary = openai_summarize(snippet)\n",
    "        summaries.append((title, summary, link))\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b86a94",
   "metadata": {},
   "source": [
    "## Semantic Similarity Ranking\n",
    "Next we rank the evidence by semantic similarity to the claim using SentenceTransformers. We encode both the claim and each summary into embeddings and compute cosine similarity. Higher scores mean more relevant evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1731e",
   "metadata": {},
   "source": [
    "3: Chain-of-Thought Reasoning with GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1d59d",
   "metadata": {},
   "source": [
    "With evidence in hand, we now prompt an OpenAI model (GPT-4 or GPT-3.5) to reason about the claim.\n",
    "We use a chain-of-thought prompt, asking the model to \"think step by step\" to make the reasoning\n",
    "explicit . We include the top evidence snippets in the prompt to ground the reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d13140b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt_chain_of_thought(claim, top_summaries):\n",
    "    # Prepare evidence text block\n",
    "    evidence_block = \"\\n\".join(f\"- \\\"{summary}\\\" (Source: {title})\" for title, summary, link in top_summaries[:3])\n",
    "    prompt = (\n",
    "        f\"You are a fact-checking assistant. Evaluate the following claim: \\n\\n\"\n",
    "        f\"\\\"{claim}\\\"\\n\\n\"\n",
    "        f\"Use the evidence below to decide if the claim is TRUE or FALSE. \"\n",
    "        f\"Provide your reasoning step by step, citing the evidence as needed.\\n\\n\"\n",
    "        f\"Evidence:\\n{evidence_block}\\n\\n\"\n",
    "        f\"Question: Is the claim true or false? Explain carefully.\"\n",
    "        f\"awnser in this format: TRUE/FALSE: REASON\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and precise factchecking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1611b",
   "metadata": {},
   "source": [
    "# 4: Final PIpeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06e921ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_check_pipeline(claim, top_k=5):\n",
    "    # 1. Search Query from claim\n",
    "    query = openai_search_query(claim).strip(\"\\\"\")\n",
    "    # 2. Retrieve evidence\n",
    "    evidence = retrieve_evidence(query, top_k=top_k)\n",
    "    if not evidence:\n",
    "        return {\"claim\": claim, \"query\": query, \"label\": \"Unknown\", \"reasoning\":\"No evidence found.\"}\n",
    "    # 3. Summarize snippets\n",
    "    summarized = summarize_snippets(evidence)\n",
    "    # 4. Ask GPT to reason\n",
    "    reasoning = ask_gpt_chain_of_thought(claim, summarized)\n",
    "    # For simplicity, assume the first word of reasoning is the label (True/False)\n",
    "    label = reasoning.split(\":\")[0].rstrip(':.')\n",
    "    return {\"claim\": claim, \"query\": query, \"label\": label, \"reasoning\": reasoning}\n",
    "\n",
    "# fact_check_pipeline(\"The Great Wall of China is visible from the Moon.\")\n",
    "\n",
    "# fact_check_pipeline(\"cristiano ronaldo is brother of ronaldo nazario.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b21a4",
   "metadata": {},
   "source": [
    "# Evaluation with LIAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa939e2",
   "metadata": {},
   "source": [
    "LIAR Dataset: Contains ~12.8K human-labeled claims from Politifact\n",
    "sites.cs.ucsb.edu\n",
    " (labels like True, Mostly False, Pants on Fire, etc.). We would map LIAR labels to a binary True/False and measure accuracy or F1 of our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911734ac",
   "metadata": {},
   "source": [
    "* LIAR Dataset is available on: https://www.cs.ucsb.edu/~william/data/liar_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc76b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "liar_dataset = []\n",
    "with open(\"datasets/liar/test.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        liar_dataset.append(row)\n",
    "liar_dataset = [data for data in liar_dataset if data[1]==\"true\" or data[1]==\"false\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29a51ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary(results, positive_label='true'):\n",
    "    tp = fp = tn = fn = 0\n",
    "    for result, y_true in results:\n",
    "        y_pred = result['label'].strip().lower()\n",
    "        y_true = y_true.strip().lower()\n",
    "\n",
    "        if y_true == positive_label:\n",
    "            if y_pred == positive_label:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if y_pred == positive_label:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    total = tp + tn + fp + fn\n",
    "    accuracy  = (tp + tn) / total if total else 0\n",
    "    precision = tp / (tp + fp)   if (tp + fp) else 0\n",
    "    recall    = tp / (tp + fn)   if (tp + fn) else 0\n",
    "    f1_score  = (2 * precision * recall / (precision + recall)\n",
    "                 if (precision + recall) else 0)\n",
    "\n",
    "    return {\n",
    "        'accuracy':  accuracy,\n",
    "        'precision': precision,\n",
    "        'recall':    recall,\n",
    "        'f1_score':  f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ab864",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for d in liar_dataset:\n",
    "    results.append((fact_check_pipeline(d[2]), d[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim': 'Over the past five years the federal government has paid out $601 million in retirement and disability benefits to deceased former federal employees.', 'query': 'federal government retirement disability benefits deceased former employees $601 million past five years', 'label': 'FALSE', 'reasoning': \"FALSE: The claim states that the federal government has paid out $601 million in retirement and disability benefits to deceased former federal employees over the past five years. However, the evidence indicates that the Federal Government's Civil Service Retirement and Disability program has made payments totaling over $601 million annually, not specifically to deceased employees. This suggests that the total payments include benefits to living retirees and possibly other categories, not exclusively to deceased former employees. Therefore, the claim is misleading and not accurate as stated.\"}\n",
      "{'claim': 'Says that Tennessee law requires that schools receive half of proceeds -- $31 million per year -- from a half-cent increase in the Shelby County sales tax.', 'query': 'Tennessee law schools receive half proceeds $31 million half-cent increase Shelby County sales tax', 'label': 'FALSE', 'reasoning': 'FALSE: The claim that Tennessee law requires schools to receive half of the proceeds from a half-cent increase in the Shelby County sales tax is not supported by the provided evidence. The evidence discusses the allocation of mixed drink tax revenue to school systems and mentions revenue increases from local option and state sales tax, but it does not specifically state that a half-cent increase in sales tax must allocate half of its proceeds to schools. Additionally, the mention of option sales tax revenue being used to support county schools does not confirm the specific claim about the half-cent increase or the $31 million figure. Therefore, the claim lacks sufficient support and is deemed false.'}\n",
      "{'claim': 'Says the unemployment rate for college graduates is 4.4 percent and over 10 percent for noncollege-educated.', 'query': 'current unemployment rate for college graduates 2023 site:gov OR site:edu', 'label': 'FALSE', 'reasoning': \"FALSE: The claim states that the unemployment rate for college graduates is 4.4 percent, but the evidence indicates that as of October 2023, the unemployment rate for recent college graduates with a bachelor's degree was 61.4 percent. This figure is significantly higher than the claim. Additionally, while the evidence does not provide a specific unemployment rate for non-college-educated individuals, it does suggest that the unemployment rates for those without a college degree are generally higher than those with a degree, but the claim's specific figures are not supported by the provided data. Therefore, the claim is false.\"}\n",
      "{'claim': \"Each year, 18,000 people die in America because they don't have health care.\", 'query': '18,000 people die in America each year due to lack of health care', 'label': 'FALSE', 'reasoning': 'FALSE: The claim states that \"each year, 18,000 people die in America because they don\\'t have health care.\" However, the evidence indicates that nearly 45,000 people die annually due to a lack of health insurance, which is significantly higher than the 18,000 figure mentioned in the claim. The study from Harvard Medical School and Cambridge Health Alliance also supports the notion that uninsured individuals face a higher risk of mortality, further emphasizing the serious consequences of lacking health insurance. Therefore, the claim is inaccurate as it underestimates the number of deaths associated with the lack of health care coverage.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for result, y_true in results[0:10]:\n",
    "    if(y_true != result['label'].lower()):\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "519ff55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6197183098591549,\n",
       " 'precision': 0.68,\n",
       " 'recall': 0.4722222222222222,\n",
       " 'f1_score': 0.5573770491803278}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_binary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20128baa",
   "metadata": {},
   "source": [
    "* As you can see in results the algorithm doesnt work good on facts that include numbers in them.\n",
    "* and the accuracy goes somewhere between CRH and UFD but with a much more cost.\n",
    "\n",
    "\n",
    "<img src=\"src/Performance-comparison-on-LIAR-dataset.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
