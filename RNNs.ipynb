{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs has not much memory, just sending output as input again. h->e->l->l->o\n",
    "We can set two outputs for our MLP, one for next word and other information, represntation or memroy of this state or word this can memorise more information we call them hidden states. on input also we get two vectors one as hidden state and one the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/RNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there many types of RNNs one to one, one to many, ... and many to many. \n",
    "* slide 16 from RNN.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can Process Non-Sequential Data with RNNs like converting image to pathces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How h(t) or y(t) is calculated.\n",
    "* slide 20 from RNN.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can ignore some states by connecting older states.\n",
    " \n",
    "Ws are all same on h, h(t+1), h(t+2) and ... .\n",
    "\n",
    "We can combine sequence to sequence models together and making networks like autoEncoder(Many-to-one + one-to-many) usages in Translation or Image Captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN DisAdvantages\n",
    "* RNNs cant be run sequantially on GPU (multiple low level API calls).\n",
    "* They are too slow.\n",
    "* they have vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are calculated recursively and consider activation function is tanh then t+n numbers between 0-1 are multiplied together so gradients get close to zero and un-meaingful at first layers.\n",
    "\n",
    "if you get more deep in this concept you can fit 10 data between 0-1 with 0.1 span, with 0.01 you can fit 100, for fitting more information you have to always shrink this span but from somewhere that will make no sense.\n",
    "\n",
    "not putting tanh causes getting too much large numbers and leading to infinite is also called as exploding gradients. solution of vanishing gradients is cliping values(limiting values to no get greater than some threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the solution of vanishing graidents is LSTM (they change the architechture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to 22_LSTMs.mp4 22:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
