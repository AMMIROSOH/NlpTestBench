{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> MAMBA </strong>\n",
    "\n",
    "The paper \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" introduces Mamba, a novel sequence modeling architecture designed to address the computational inefficiencies of Transformers, especially when handling long sequences.\n",
    "\n",
    "* Traditional Transformers, which underpin many deep learning applications, face challenges with long sequences due to their quadratic computational complexity and limited context window. While various subquadratic-time architectures have been proposed, they often underperform in key areas like language processing. The authors identify that a significant weakness of these models is their inability to perform content-based reasoning, it seems that mamaba is 5 times faster on inference throughput vs Best transformers.\n",
    "\n",
    "* MAMBA Computation grows linearly and Transformers grow Expontially.\n",
    "\n",
    "* Unlike LSTMs that each block should wait for output of previous Block, MAMBAs gets a global hidden state value and blocks dont need to wait for eachother.\n",
    "\n",
    "> note: <strong> mamba is a mixup of LSTMs and Attentions that is inherited from a old Sequence Modeling that were not being used these days. </strong>\n",
    "\n",
    "* Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
    "* https://arxiv.org/pdf/2312.00752\n",
    "* VMamba: Visual State Space Model\n",
    "* https://arxiv.org/abs/2401.10166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/MAMBA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
    "\n",
    "## **Core Ideas**\n",
    "\n",
    "### **Linear State Space Models (SSMs):**\n",
    "- Mamba builds on the foundation of linear state space models, which efficiently model sequences and capture long-term dependencies.\n",
    "- **SSM Formula:**\n",
    "  ```math\n",
    "  h_t = A h_{t-1} + B x_t, \\quad y_t = C h_t + D x_t\n",
    "  ```\n",
    "  Where:\n",
    "  - \\(A, B, C, D\\) are learned parameters.\n",
    "  - \\(x_t\\): Input at time \\(t\\).\n",
    "  - \\(h_t\\): Hidden state.\n",
    "\n",
    "### **Selective State Spaces (SSMs with Inputs as Parameters):**\n",
    "- In Mamba, **parameters like \\(A, B, C\\)** are **functions of the input \\(x_t\\)**:\n",
    "  ```math\n",
    "  A_t = f_A(x_t), \\quad B_t = f_B(x_t), \\quad C_t = f_C(x_t)\n",
    "  ```\n",
    "- This dynamic parameterization conditions the state propagation on the input, enabling efficient handling of discrete modalities like language.\n",
    "\n",
    "### **Linear Time Complexity:**\n",
    "- Mamba achieves linear complexity \\(O(T)\\) with sequence length \\(T\\) by leveraging a **parallel recurrent algorithm**, avoiding attention mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Mamba Works**\n",
    "\n",
    "### **Dynamic Parameterization:**\n",
    "- For each time step \\(t\\):\n",
    "  ```math\n",
    "  A_t = f_A(x_t), \\quad B_t = f_B(x_t), \\quad C_t = f_C(x_t)\n",
    "  ```\n",
    "- Lightweight feedforward networks generate these parameters dynamically.\n",
    "\n",
    "### **Selective Information Propagation:**\n",
    "- Mamba selectively **propagates** or **resets** the state \\(h_t\\), inspired by gating structures like in LSTMs.\n",
    "\n",
    "### **Efficient Recurrence:**\n",
    "- **Parallel Algorithm**: Processes sequences in block-parallel fashion for high throughput, avoiding sequential bottlenecks of RNNs.\n",
    "\n",
    "### **Output Computation:**\n",
    "- Output combines historical context and immediate input:\n",
    "  ```math\n",
    "  y_t = C_t h_t + D x_t\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Optimizations in Mamba**\n",
    "\n",
    "1. **No Attention or MLP Blocks:**\n",
    "   - Simplified architecture eliminates these costly components.\n",
    "\n",
    "2. **Parallel Algorithm:**\n",
    "   - Balances recurrence with hardware-friendly parallelization.\n",
    "\n",
    "3. **Input-Specific Computations:**\n",
    "   - Dynamically modulates state propagation for better adaptability.\n",
    "\n",
    "---\n",
    "\n",
    "## **Performance Advantages**\n",
    "\n",
    "1. **Efficiency:**\n",
    "   - Linear scaling with \\(T\\), handling sequences up to 1M tokens.\n",
    "   - 5x inference throughput compared to Transformers.\n",
    "\n",
    "2. **Accuracy:**\n",
    "   - Matches or outperforms Transformers in language modeling.\n",
    "   - Excels in multimodal tasks (e.g., audio, genomics).\n",
    "\n",
    "3. **Parameter Efficiency:**\n",
    "   - Similar performance with fewer parameters than Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Mamba Works Well**\n",
    "\n",
    "1. **Selective State Spaces:**\n",
    "   - Focuses on relevant sequence parts, ignoring irrelevant ones.\n",
    "\n",
    "2. **Long-Term Dependencies:**\n",
    "   - Recurrence inherently captures these without explicit positional encodings.\n",
    "\n",
    "3. **Hardware Awareness:**\n",
    "   - Optimized for modern accelerators (GPUs/TPUs).\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications of Mamba**\n",
    "\n",
    "- **Language Modeling:**\n",
    "  - Outperforms Transformers of equivalent size and matches models twice its size.\n",
    "- **Audio and Time-Series Processing:**\n",
    "  - Handles long-range dependencies in time-series data efficiently.\n",
    "- **Genomics:**\n",
    "  - Processes long genetic sequences beyond Transformer context limits.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Differences with Transformers**\n",
    "\n",
    "| Feature                | Transformers                | Mamba                     |\n",
    "|------------------------|-----------------------------|---------------------------|\n",
    "| Complexity             | \\(O(T^2)\\)                 | \\(O(T)\\)                  |\n",
    "| Long Dependencies      | Limited (context size)      | Efficient (state-based)   |\n",
    "| Parameter Sharing      | Fixed across tokens         | Input-conditioned         |\n",
    "| Mechanism              | Attention                  | Selective State Spaces    |\n",
    "| Modality Adaptability  | High (for language)         | High (multimodal)         |\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "Mamba combines:\n",
    "- **State Space Models** for efficient sequence modeling.\n",
    "- **Selective Mechanisms** for content-based reasoning.\n",
    "- **Parallel Algorithms** for practical scalability.\n",
    "\n",
    "It offers a **streamlined, efficient, and scalable alternative** to Transformers, excelling across multiple modalities with state-of-the-art performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"src/MAMBAModeularImplementation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mamba in Neural Networks\n",
    "\n",
    "Mamba is designed to be highly modular and can be integrated into neural network architectures in a flexible way. Its linear-time state space mechanism makes it ideal for tasks requiring efficient sequence modeling across various modalities. Here's how Mamba can be used in neural network architectures, along with guidelines for modularity and block design.\n",
    "\n",
    "---\n",
    "\n",
    "## **Using Mamba in Neural Networks**\n",
    "Mamba functions as a **sequence modeling block** that can replace traditional Transformer layers or recurrent units (like LSTMs or GRUs). It operates as a standalone component that processes sequential inputs and can be stacked or combined with other modules.\n",
    "\n",
    "### **Key Roles in Architectures**\n",
    "1. **As a Backbone:**\n",
    "   - Mamba can serve as the main sequence processor in architectures for tasks like language modeling, audio processing, or time-series prediction.\n",
    "   - Example: Replace Transformer layers in GPT-like models with Mamba layers.\n",
    "\n",
    "2. **As a Hybrid Component:**\n",
    "   - Mamba can be combined with Transformer blocks, CNNs, or other architectures to handle specific aspects of a task, such as long-range dependencies or efficient feature extraction.\n",
    "\n",
    "3. **For Multimodal Tasks:**\n",
    "   - Mamba's selective state spaces adapt well to multiple data types (e.g., text, audio, genomics), making it suitable for architectures handling diverse inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Modularity in Mamba**\n",
    "To use Mamba effectively, it should be modular, enabling it to integrate with existing neural network layers.\n",
    "\n",
    "### **Mamba Block Design**\n",
    "Each **Mamba block** consists of the following components:\n",
    "\n",
    "1. **Input Encoding Layer:**\n",
    "   - Converts raw inputs into a suitable representation.\n",
    "   - Can be an embedding layer (for text) or a convolutional layer (for audio).\n",
    "\n",
    "   ```math\n",
    "   z_t = \\text{Embedding}(x_t) \\quad \\text{or} \\quad z_t = \\text{Conv}(x_t)\n",
    "   ```\n",
    "\n",
    "2. **Dynamic Parameter Generation:**\n",
    "   - Lightweight feedforward layers compute the dynamic parameters \\(A_t, B_t, C_t, D_t\\) for each time step \\(t\\) based on the input.\n",
    "   - These functions can be shared across tokens or customized for each one.\n",
    "\n",
    "   ```math\n",
    "   A_t = f_A(z_t), \\quad B_t = f_B(z_t), \\quad C_t = f_C(z_t)\n",
    "   ```\n",
    "\n",
    "3. **State Update Module (Core SSM):**\n",
    "   - The core selective state space computation is performed here. It updates the hidden state \\(h_t\\) and computes the output \\(y_t\\) for each time step.\n",
    "   - Parallelized recurrence ensures efficiency.\n",
    "\n",
    "   ```math\n",
    "   h_t = A_t h_{t-1} + B_t z_t, \\quad y_t = C_t h_t + D_t z_t\n",
    "   ```\n",
    "\n",
    "4. **Residual Connections (Optional):**\n",
    "   - Residual connections can be added to stabilize training, similar to Transformer blocks.\n",
    "\n",
    "   ```math\n",
    "   y_t = y_t + \\text{Residual Input}\n",
    "   ```\n",
    "\n",
    "5. **Normalization and Output Layer:**\n",
    "   - Apply layer normalization and pass the output to the next Mamba block or the task-specific head.\n",
    "\n",
    "   ```math\n",
    "   y_t = \\text{LayerNorm}(y_t)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Stacking Mamba Blocks**\n",
    "- Multiple Mamba blocks can be stacked to handle complex dependencies and hierarchical representations.\n",
    "- Each block operates in parallel, processing the input sequence as a whole, but respects recurrence internally for efficient state updates.\n",
    "\n",
    "### **Single Mamba Block**\n",
    "```\n",
    "[Input] → [Dynamic Parameter Generation] → [State Update] → [Residual + Normalization] → [Output]\n",
    "```\n",
    "\n",
    "### **Stacked Mamba Architecture**\n",
    "```\n",
    "[Input] → [Embedding/Encoding] → [Mamba Block 1] → [Mamba Block 2] → ... → [Task Head]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Hybrid Architectures with Mamba**\n",
    "Mamba can complement other architectures, such as Transformers or convolutional networks:\n",
    "\n",
    "### **Hybrid Transformer-Mamba Architecture**\n",
    "- Use Mamba for handling long-range dependencies and Transformers for local interactions.\n",
    "- Alternate Mamba and Transformer blocks in a stacked architecture.\n",
    "\n",
    "```\n",
    "[Input] → [Embedding] → [Mamba Block] → [Transformer Block] → [Mamba Block] → [Task Head]\n",
    "```\n",
    "\n",
    "### **Convolution + Mamba for Audio/Time-Series**\n",
    "- Combine convolutional layers for local feature extraction with Mamba for temporal modeling.\n",
    "\n",
    "```\n",
    "[Input] → [Conv Layers] → [Mamba Blocks] → [Task Head]\n",
    "```\n",
    "\n",
    "### **Multimodal Architecture**\n",
    "- Use Mamba for processing sequential data (e.g., text or audio) and combine it with other branches for handling non-sequential data (e.g., images).\n",
    "\n",
    "```\n",
    "[Image Input] → [CNN]\n",
    "[Text Input] → [Mamba Blocks]\n",
    "[Audio Input] → [Mamba Blocks]\n",
    "→ [Fusion Layer] → [Task Head]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Mamba Block Implementation (Pseudocode)**\n",
    "```python\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MambaBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Dynamic parameter generators\n",
    "        self.f_A = nn.Linear(input_dim, hidden_dim)\n",
    "        self.f_B = nn.Linear(input_dim, hidden_dim)\n",
    "        self.f_C = nn.Linear(input_dim, hidden_dim)\n",
    "        self.f_D = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, batch_size, input_dim = x.shape\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)  # Initial hidden state\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            z_t = x[t]\n",
    "\n",
    "            # Compute dynamic parameters\n",
    "            A_t = self.f_A(z_t)\n",
    "            B_t = self.f_B(z_t)\n",
    "            C_t = self.f_C(z_t)\n",
    "            D_t = self.f_D(z_t)\n",
    "\n",
    "            # Update hidden state and compute output\n",
    "            h_t = A_t * h_t + B_t * z_t\n",
    "            y_t = C_t * h_t + D_t * z_t\n",
    "\n",
    "            outputs.append(y_t)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        outputs = self.layer_norm(outputs)\n",
    "        return outputs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "- Mamba is modular and flexible, suitable for standalone use or as part of hybrid architectures.\n",
    "- A single block consists of dynamic parameter generation, state updates, and optional residual/normalization layers.\n",
    "- You can stack Mamba blocks or combine them with other layers to handle a wide variety of tasks, from language modeling to audio processing.\n",
    "- Hybrid architectures with Mamba and Transformers or CNNs can exploit the strengths of both approaches.\n",
    "\n",
    "Let me know if you'd like help refining an architecture using Mamba or implementing it for a specific task!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
