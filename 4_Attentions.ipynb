{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "Attention came from bottleNeck problem of Encoder-Decoder architectures that we Assign attention weight to each word, to know how much \"attention\" the model should pay to each word.\n",
    "\n",
    "* Neural Machine Translation by Jointly Learning to Align and Translate 2014, introducing Attention from BiDirectional RNN Encoder-Decoders.\n",
    "* https://arxiv.org/abs/1409.0473\n",
    "* Attention Is All You Need 2017, introducing self-attention and transformers\n",
    "* https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At First they added a weight to out put of each RNN block called \"RNN attention mechanism\". then they upgraded it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/RNN-Attention.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems\n",
    "* Attention become almost diagonal and not very flexible\n",
    "* because of RNN blocks cant be run parrarel on GPU\n",
    "* no convolution\n",
    "\n",
    "Transformers that rely on the self-attention, process whole sequence at once, so they no more use LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# at Attention Is All You Need\n",
    "they introduced self-attention, cross-attention, multi-head-attention and how to use attentions as stack of layers.\n",
    "\n",
    "at very basic form of self-attention we can ignore learnability of attention weights and set cosine similarity of inputs that are our Embedding of words as attentions to each word. and their inlearnability are they disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention(Q, K, V ) = softmax(Q*KT/√dk)*V \n",
    "for fixing inlearnability they added three learnable matrices to attention. and the formula is called Scaled Dot Production.\n",
    "\n",
    "<img src=\"src/Attention.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√dk is for scaling the dimenstions of matrices so softmax can normalize more smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as each attention run parrarel we can add multiple attentions at once calling multihead attention.\n",
    "\n",
    "Sequential models are still being used like LSTMS in convex2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "we make transformers from multi head attentions with an encoder-decoder architecture and the second attention in decoder part gets v, k from encoder and q from its masked attention on output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder part is added for translation task so output is involved, and maske is used there to prevent cheating (looking forward words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/Transformers.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■ There has been an array of Transformer based architectures such as BERT, SpanBERT,\n",
    "Transformer-XL, XLNet, GPT-2, etc getting released frequently for the past couple of\n",
    "years.\n",
    "\n",
    "■ The OpenAI’s GPT-3 had taken the internet by storm with its ability to perform\n",
    "extremely well on tasks such as QA, Comprehension, even Programming\n",
    "\n",
    "■ During training, we use a loss function such as cross-entropy loss to compare the\n",
    "generated output probability distribution to the target sequence.\n",
    "\n",
    "■ The probability distribution gives the probability of each word occurring in that position.\n",
    "\n",
    "■ As usual, the loss is used to compute gradients to train the Transformer via\n",
    "backpropagation ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■ RNNs and their relatives, LSTMs and GRUs had two main limitations:\n",
    "\n",
    "▶ It was challenging to deal with long-range dependencies between words\n",
    "that were spread far apart in a long sentence.\n",
    "\n",
    "▶ They process the input sequence sequentially one word at a time , which\n",
    "means that it cannot do the computation for time-step t until it has\n",
    "completed the computation for time-step t—1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/TransformersInPractice.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "batch normalization is not used instead normalization is applied to each channel(layed in 3d preview) of attentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding \n",
    "is added afte Embedding for including position of each word in contex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■ Transformers don’t use RNNs and all words in a sequence are input in parallel.\n",
    "\n",
    "■ This is its major advantage over the RNN architecture, but it means that the position\n",
    "information is lost, and has to be added back in separately.\n",
    "\n",
    "■ The Position Encoding is computed independently of the input sequence.\n",
    "\n",
    "■ These are fixed values that depend only on the max length of the sequence. These\n",
    "constants are computed as follows:\n",
    "P E(pos,2i) = sin(pos/100002i/dmodel)\n",
    "P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
    "where,\n",
    "\n",
    "▶ pos is the position of the word in the sequence.\n",
    "\n",
    "▶ dmodel is the length of the encoding vector\n",
    "\n",
    "▶ i is the index value into this vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention os Self-Supervised\n",
    "## NOT MUCH LABELD DATA NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are Attention Mechanisms Important?\n",
    "## Attention mechanisms have become indispensable in various deep-learning applications due to their ability to address some critical challenges:\n",
    "\n",
    "* Long Sequences: Traditional neural networks struggle with processing long sequences, such as translating a paragraph from one language to another. Attention mechanisms allow models to focus on the relevant parts of the input, making them more effective at handling lengthy data.\n",
    "* Contextual Understanding: In tasks like language translation, understanding the context of a word is crucial for accurate translation. Attention mechanisms enable models to consider the context by assigning different attention weights to each word in the input sequence.\n",
    "* Improved Performance: Models equipped with attention mechanisms often outperform their non-attention counterparts. They achieve state-of-the-art results in tasks like machine translation, image classification, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The name \"transformer\" for models using attention layers comes from the influential paper \"Attention Is All You Need\" by Vaswani et al. (2017). The term \"transformer\" reflects the model's architecture and its ability to \"transform\" sequences of data through a series of attention-based layers.\n",
    "\n",
    "## Here are key reasons why the name \"transformer\" was chosen:\n",
    "\n",
    "* Attention Mechanism: The core innovation of the transformer model is the self-attention mechanism. This allows the model to weigh the importance of different words in a sentence, regardless of their position, enabling it to capture long-range dependencies and relationships more effectively than previous models.\n",
    "\n",
    "* Sequence Transformation: The model transforms input sequences into output sequences. In the context of natural language processing (NLP), this means transforming an input sentence into an output sentence, such as translating from one language to another.\n",
    "\n",
    "* Layered Architecture: Transformers are composed of multiple layers of self-attention and feed-forward neural networks. Each layer processes the input and transforms it into a more abstract representation, progressively refining the output through successive transformations.\n",
    "\n",
    "* Scalability: The architecture is highly scalable, allowing for the use of large amounts of data and computational resources to improve performance. This transformation of data at scale is a key feature of the model.\n",
    "\n",
    "* Shift from RNNs/CNNs: Prior to transformers, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) were commonly used for sequence modeling. The transformer model represents a significant shift in approach, using attention mechanisms without recurrence or convolution, thus transforming the field of deep learning.\n",
    "\n",
    "In summary, the name \"transformer\" encapsulates the model's ability to apply attention mechanisms to transform input sequences into more useful representations, enabling powerful and flexible modeling of sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Size\n",
    "### Context size is a huge bottleneck for language models because its o(n^2), K(n)*Q(n) so these are some approches for tackling this limit\n",
    "* Sparse Attention Mechanism\n",
    "* Blockwise Attention\n",
    "* Linformer\n",
    "* Reformer\n",
    "* Ring attention\n",
    "* Longformer\n",
    "* Adaptive Attention Spam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
